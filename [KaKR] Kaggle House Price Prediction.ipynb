{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# **2019 2nd ML month with KaKR**\n**House Price Prediction Challenge**\n현우님의 커널을 베이스로 다른 커널들을 참고하며 공부중입니다 \n    * https://www.kaggle.com/chocozzz/house-price-prediction-eda-updated-2019-03-12/notebook\n\n- **[Updated]**  2019.03.22 - 기존 작성되지 않은 Description 추가 및 Scaling vs. Normalization을 추가했습니다.\n- **[Updated]**  2019.03.25 - 예제를 제외한 부분에서 기존 numpy를 통해 작성한 부분을 변경했습니다(공부목적의 이유). 또한, bathrooms 설명 오류 부분을 수정하였습니다.\n- **[Updated]**  2019.04.12 - 코드 상세 설명 추가 및 파생변수 부분을 수정하였습니다. Randomforest 내 RandomizedSearch 추가 및 XGBoost 모델을 추가하였습니다. \n- **[Updated]**  2019.04.15 - lgbm 모델을 추가하였습니다. 파생변수 추가 및 그로 인한 코드 수정이 있습니다."},{"metadata":{"_uuid":"087e3560379e079b2338924a0a182398df4bcde1"},"cell_type":"markdown","source":"# **목차**\n- <a href='#1'>1. Data Fields</a>  \n- <a href='#2'>2. Exploratory Data Analysis(EDA)</a>  \n    - <a href='#21'>2-1. 결측치 확인</a>\n    - <a href='#22'>2-2. 변수별 유니크 값 개수 확인</a>\n    - <a href='#23'>2-3. 종속변수(Y)</a>\n    - <a href='#231'>2-3-1. Scaling vs. Normalization</a>\n    - <a href='#232'>2-3-2. 정규화(Normalization)</a>\n    - <a href='#24'>2-4. 독립변수(X)</a>\n    - <a href='#241'>2-4-1. 정규화(Normalization)</a>\n- <a href='#3'>3. 이상치 처리</a>  \n    - <a href='#32'>3-1. 독립변수(X)</a>\n- <a href='#4'>4. 파생변수 생성</a>  \n- <a href='#5'>5. 모델링</a>  \n    - <a href='#51'>5-1. Linear Regression</a>\n    - <a href='#52'>5-2. Random Forest</a>\n    - <a href='#53'>5-3. XGBoost</a>"},{"metadata":{"_uuid":"204f5fe3730efb360681a53958c37116b0cd8fb3"},"cell_type":"markdown","source":"--------------------------------------------------------------------------------------------------------------------------\n\n# **<a id='1'> 1. Data Fields </a>**\n\n* ID : 집을 구분하는 번호\n* date : 집을 구매한 날짜\n* price : 집의 가격(Target variable)\n* bedrooms : 침실의 수\n* bathrooms : 화장실의 수\n* sqft_living : 주거 공간의 평방 피트(면적)\n* sqft_lot : 부지의 평방 피트(면적)\n* floors : 집의 층 수\n* waterfront : 집의 전방에 강이 흐르는지 유무 (a.k.a. 리버뷰)\n* view : 집이 얼마나 좋아 보이는지의 정도\n* condition : 집의 전반적인 상태\n* grade : King County grading 시스템 기준으로 매긴 집의 등급\n* sqft_above : 지하실을 제외한 평방 피트(면적)\n* sqft_basement : 지하실의 평방 피트(면적)\n* yr_built : 지어진 년도\n* yr_renovated : 집을 재건축한 년도\n* zipcode : 우편번호\n* lat : 위도\n* long : 경도\n* sqft_living15 : 2015년 기준 주거 공간의 평방 피트(면적, 집을 재건축했다면, 변화가 있을 수 있음)\n* sqft_lot15 : 2015년 기준 부지의 평방 피트(면적, 집을 재건축했다면, 변화가 있을 수 있음)\n--------------------------------------------------------------------------------------------------------------------------"},{"metadata":{"_uuid":"aa387d001a04215fead4f5fabc12cf412edcb3cb"},"cell_type":"markdown","source":"가장 먼저, Data fields의 변수들을 보며 데이터를 이해하고,  설명을 보며 파생변수가 될 수 있는 것들을 체크했습니다.\n    * bedrooms와 bathrooms는 각 room의 개수를 나타내고 있어 비슷한 특성을 띈다 파악할 수 있습니다, \n      이를 합하여 bedroom과 bathroom을 변수 생성할 수 있을 것입니다\n    * sqft 관련 변수들의 경우, 면적의 영향이 있기에 면적이 클 경우 가격이 높을 수 밖에 없는 관계가 \n      있으므로, row간 상대적인 비교를 위해서는 면적의 영향을 줄이는게 좋을 듯 합니다. 이를 위한 방안은 추후 생각 해볼 수 있을 것입니다.\n    * 모든 면적을 더하고, 이를 집값으로 나누어 면적 당 평균가 변수를 생성할 수 있을 것이라 생각합니다.\n    * sqft_living15 - sqft_living을 하여 변화의 차이를 파생변수로 생성 할 수 있을 것이라 생각합니다."},{"metadata":{"trusted":true,"_uuid":"f98917a3a510147443aeb6fb71c90cfaed10decf"},"cell_type":"code","source":"# Loading packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import stats\nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4535702a48b3ed96bfcae8fab8c6d97e6bae7b71"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcb903a903b9120ad03d2077a6aabf5a5d05d6a9"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c4b6a53737e3c0feb1e60ce01fae3e3364c52de"},"cell_type":"markdown","source":"head를 통해 간략히 데이터를 볼 때, 다음을 알 수 있었습니다.\n  * sqft_living에서 sqft_living15로 가며 면적이 다소 증가함이 보입니다.\n  * sqft_lot은 sqft_living과 다르게 sqft_lot15로 가며 다소 감소했음을 알 수 있습니다."},{"metadata":{"_uuid":"30f5868c36dd46140effc430fc37770aecc16d67"},"cell_type":"markdown","source":"# **<a id='2'> 2. Exploratory Data Analysis(EDA) </a>**\n\n--------------------------------------------------------------------------------------------------------------------------"},{"metadata":{"_uuid":"37e3c939946ac070c582031e27751f6a55f84c31"},"cell_type":"markdown","source":"### **<a id='21'> 2-1. 결측치 확인 </a>**"},{"metadata":{"trusted":true,"_uuid":"86ba051e88f968afe86bcf44ce7ad39acca49b1e"},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d3b3f17919aa441e19a29a1190e8efc016d75f0"},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6773a8446e5e7edba9c0a731eb94e7f8e1c53f5b"},"cell_type":"markdown","source":"* 본 competition의 데이터는 결측치가 없이 clean하게 제공되었음을 확인하였습니다."},{"metadata":{"_uuid":"eb7836090928f1d55eaefc4e7688e3fb732df714"},"cell_type":"markdown","source":"### **<a id='22'> 2-2. 변수별 유니크 값 개수 확인 </a>**"},{"metadata":{"trusted":true,"_uuid":"549cae76d41d1b154d74d84fda379ffe86de2160"},"cell_type":"code","source":"train.nunique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe2f6182ba5bcb54bb06d123fedbf3fd18dc8211"},"cell_type":"markdown","source":"  * waterfront는 이진 변수임을 알 수 있습니다.\n  * condition, view는 정도를 나타내는 Rank변수로 예상됩니다.\n\n자세한 사항은 앞으로 EDA를 통해 변수들을 자세히 알아보도록 하겠습니다."},{"metadata":{"_uuid":"6eacf7d78ab63977aa6291256af12c36e3696f2d"},"cell_type":"markdown","source":"### **<a id='23'> 2-3. 종속변수(Y) </a>**"},{"metadata":{"trusted":true,"_uuid":"ce9abf29c44521e3313efccb4f03f25ed7be5ce6"},"cell_type":"code","source":"train['price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ac296e97fa5a30a42ddff422cfd04f96800ed3d"},"cell_type":"markdown","source":"* min값과 1분위수(first quartile)값의 자리수가 다름을 알 수 있습니다.\n* 3분위수(third quantile)값과 max값의 자리수가 다름을 알 수 있습니다. \n* min값과 1분위수, 각 분위수들의 차이보다도 75%(3Q)에서 max값의 차이가 더욱 큼을 알 수 있습니다."},{"metadata":{"trusted":true,"_uuid":"c4377b3e7d0c6a9ceac186ad6ea591b33a00da47","scrolled":true},"cell_type":"code","source":"#histogram\nf, ax = plt.subplots(figsize = (8, 6))\nsns.distplot(train['price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09bbb2f4b8b4aa70670d03015e7e4a6d7f7eb085"},"cell_type":"code","source":"train['price'].skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fdc2a06b44956c43f93b105a85a55ba96b4553e"},"cell_type":"code","source":"train['price'].kurt()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73778856ba2e9d0d0fd7399d714a0564d17ae405"},"cell_type":"markdown","source":"* 왜도(skewness)는 정규분포와 비교하여 왼쪽으로 꼬리가 길수록(left-skewed), 더욱 큰 음수값을 가지며\n  정규분포와 비교하여 오른쪽으로 꼬리가 길수록(right-skewed), 양의 값이 더욱 커집니다.\n* 첨도(kurtosis)값은 강하게 볼때는 2이내 (k<2), 완화하여 볼 경우 3이내(k<3)까지를 정규분포에 가깝게 평탄하다 해석합니다.\n\n#### 위의 histogram 및 skew, kurt값을 통해 종속변수가 right-skewed 되어있음을 확인하여 skew를 완화 해주고자 합니다."},{"metadata":{"_uuid":"6e5eadf7601b457adb2e8033899cbb4b38029125"},"cell_type":"markdown","source":"--------------------------------------------------------------------------------------------------------------------------\n** - data의 skew 완화를 설명하기 이전, 다소 헷갈릴 수 있는 Scaling과 Normalization의 정의를 알아보고자 합니다.**\n\n* 해당 정의는 https://www.kaggle.com/jfeng1023/data-cleaning-challenge-scale-and-normalize-data를 참조하였습니다.\n\n--------------------------------------------------------------------------------------------------------------------------"},{"metadata":{"_uuid":"a1682b9848adae25cf872ebe0f54143f7ce36a86"},"cell_type":"markdown","source":"## **<a id='231'> 2-3-1. Scaling vs. Normalization </a>**\nOne of the reasons that it's easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and, to make it even more confusing, they are very similar! In both cases, you're transforming the values of numeric variables so that the transformed data points have specific helpful properties. **The difference is that, in scaling, you're changing the range of your data while in normalization you're changing the shape of the distribution of your data**. Let's talk a little more in-depth about each of these options"},{"metadata":{"_uuid":"2a0e450e5a105045d05a661421d448a09df48f7b"},"cell_type":"markdown","source":"## **Scaling**\nThis means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1. You want to scale data when you're using methods based on measures of how far apart data points, like support vector machines, or SVM or k-nearest neighbors, or KNN. With these algorithms, a change of \"1\" in any numeric feature is given the same importance.\n\nFor example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\n\nBy scaling your variables, you can help compare different variables on equal footing. To help solidify what scaling looks like, let's look at a made-up example. (Don't worry, we'll work with real data in just a second, this is just to help illustrate my point.)"},{"metadata":{"trusted":true,"_uuid":"8502726d82ca1a630fc9ecb343ddb0dece3cf701"},"cell_type":"code","source":"# Scaling Example\n# generate 1000 data points randomly drawn from an exponential distribution\nfrom mlxtend.preprocessing import minmax_scaling\nimport numpy\noriginal_data = numpy.random.exponential(size = 1000)\n\n# mix-max scale the data between 0 and 1\nscaled_data = minmax_scaling(original_data, columns = [0])\n\n# plot both together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(scaled_data, ax=ax[1])\nax[1].set_title(\"Scaled data\")\ndel numpy #import numpy를 해제","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13a5624e09e7d3b734b1da706fdd273b03f5a8b7"},"cell_type":"markdown","source":"* 예시에서 데이터의 shape 변화 없이 0 to 8이던 범위를 0 to 1로 변화시켜주었습니다.\n* 이와 같이, **shape의 변화 없이 range 만을 조정하는 것이 scaling입니다.**"},{"metadata":{"_uuid":"f6f39fc48800418c57d39e0d4dde0d00ece2a7a5"},"cell_type":"markdown","source":"## **Normalization**\nScaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n\n> **Normal distribution**: Also known as the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n\nIn general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)\n\nThe method were using to normalize here is called the Box-Cox Transformation. Let's take a quick peek at what normalizing some data looks like:"},{"metadata":{"trusted":true,"_uuid":"4faa9c91b1309c549158e8a669c72f5b766f864b"},"cell_type":"code","source":"# normalize the exponential data with boxcox\nnormalized_data = stats.boxcox(original_data)\n\n# plot both together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(normalized_data[0], ax=ax[1])\nax[1].set_title(\"Normalized data\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a57a29cc0bb283c0ca47dff7dcf229ee63849b1"},"cell_type":"markdown","source":"* 기존 right-skewed된 original data에서, 처리를 통해 range의 변화가 일어날 뿐 아니라 bell-curve(=normal distribution)의 형태로 shape가 변형되었습니다.\n* 이와 같이, **분포가 변형되는(shape가 변형되는)과정에서 range의 변화가 일어나는 것이 Normalization입니다.**"},{"metadata":{"_uuid":"0f75ffefb56070d6fb56f1ef742e32d4fed31625"},"cell_type":"markdown","source":"### **<a id='232'> 2-3-2. 정규화(Normalization) </a>**\n#### **본래의 데이터로 돌아와서,**  right-skewed된 종속변수(Y)를 normalize해주기 위하여 square-root, fourth-root, log transformation을 사용하며 이를 비교하도록 하겠습니다."},{"metadata":{"trusted":true,"_uuid":"94749b59c2ea5acf6bd0fdabebbee911059aa1ac"},"cell_type":"code","source":"#histogram\n\nfig = plt.figure(figsize = (8, 6))\n\nfig.add_subplot(1, 3, 1)\nsns.distplot(sp.sqrt(train['price'])).set_title('square-root') # square root transformation\n\nfig.add_subplot(1, 3, 2)\nsns.distplot(train['price'] ** (1/float(4.0))).set_title('fourth-root') # fourth square root transformation\n\nfig.add_subplot(1, 3, 3)\nsns.distplot(sp.special.log1p(train['price'])).set_title('log1p') # log1p transformation","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7c32e77713a455f771de89e7223b5a96318c022"},"cell_type":"markdown","source":"#### * 종속변수(price)와 독립변수(sqft_living(주거 공간의 평방 피트(면적)))을 통한 독립변수(Y)의 Normalization 전후 비교"},{"metadata":{"trusted":true,"_uuid":"2c06a7937a4a032bdbb9d895b187c43657e13e4c"},"cell_type":"code","source":"# price에 square-root를 취함\ndata = pd.concat([train['price'], train['sqft_living']], axis = 1)\nf, ax = plt.subplots(figsize = (8, 6))\nfig = sns.regplot(x = 'sqft_living', y = 'price', data = data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df8d0493ed87525f69443712d892f55f2ece2652"},"cell_type":"code","source":"# price에 fourth-root를 취함\ndata = pd.concat([train['price'] ** (1/float(4.0)), train['sqft_living']], axis = 1)\nf, ax = plt.subplots(figsize = (8, 6))\nfig = sns.regplot(x = 'sqft_living', y = 'price', data = data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7346d011232b542724c65d241af47dd4059977b3"},"cell_type":"markdown","source":"* square root보다는 fourth-root transformation을 함이 맞으나, log1p와 비교해서보자"},{"metadata":{"trusted":true,"_uuid":"d8ba41f0b6c48f9e56d31e691ae223bf2c6c32f0","scrolled":false},"cell_type":"code","source":"fig = plt.figure(figsize = (8, 6))\n\nfig.add_subplot(1, 3, 1)\ndata = pd.concat([pd.DataFrame(sp.sqrt(train['price'])), train['sqft_living']], axis = 1)\ndata.columns.values[0] = 'price'\nreg = sns.regplot(x = 'sqft_living', y = 'price', data = data).set_title('square-root transformation')\n\nfig.add_subplot(1, 3, 2)\ndata = pd.concat([(train['price'] ** 1/float(4.0)), train['sqft_living']], axis = 1)\nreg = sns.regplot(x = 'sqft_living', y = 'price', data = data).set_title('fourth-root transformation')\n\nfig.add_subplot(1, 3, 3)\ndata = pd.concat([sp.special.log1p(train['price']), train['sqft_living']], axis = 1)\nreg = sns.regplot(x = 'sqft_living', y = 'price', data = data).set_title('log1p transformation')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48142dee59e466070048364d6b0fd240675eab03"},"cell_type":"markdown","source":"#### 이러한 비교를 통해, **독립변수에 log transformation**을 사용함이 옳다고 판단하여 log1p로 진행하고자 합니다.\n* log1p는 log(1+x)로서, log 0이 발생할 경우를 대비하여 log transformation을 대신하여 practical하게 사용됩니다,"},{"metadata":{"trusted":true,"_uuid":"db3f4be6dbaca5681aa3f25914e6d94859a8f3cc"},"cell_type":"code","source":"train['price'] = sp.special.log1p(train['price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train data의 price에 log transformation을 해주었으므로, 추후 모델을 통해 예측된 최종값에 exponential을 취해 원래의 값으로 제출해야함을 잊지 말아야 합니다."},{"metadata":{"_uuid":"68d4e544380f4c456fe79e6947f0326c856da48a"},"cell_type":"markdown","source":"### **<a id='24'> 2-4. 독립변수(X) </a>**\n* 범주형 변수의 시각화는 boxplot\n* 연속형 변수의 시각화는 scatter plot을 사용하여 탐색하고자 합니다."},{"metadata":{"trusted":true,"_uuid":"96efd926dbd8341dbce92c48117b946ab66af7b8"},"cell_type":"code","source":"# correlation이 높은 상위 10개의 heatmap\n# continuous + sequential variables --> spearman\n# abs는 반비례관계도 고려하기 위함\n# https://www.kaggle.com/junoindatascience/let-s-eda-it 준호님이 수정해 준 코드로 사용하였습니다. \n\ncor_abs = abs(train.corr(method='spearman')) \ncor_cols = cor_abs.nlargest(n=10, columns='price').index # price과 correlation이 높은 column 10개 뽑기(내림차순)\n# spearman coefficient matrix\ncor = sp.array(sp.stats.spearmanr(train[cor_cols].values))[0] # 10 x 10\nprint(cor_cols.values)\nplt.figure(figsize=(10,10))\nsns.set(font_scale=1.25)\nsns.heatmap(cor, fmt='.2f', annot=True, square=True , annot_kws={'size' : 8} ,xticklabels=cor_cols.values, yticklabels=cor_cols.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1e2bd5ded0d4ef6e397364bac997ab6fc4cd48e"},"cell_type":"markdown","source":"### price와 상관관계가 큰 상위 9개를 기준으로 먼저 EDA를 진행하도록 하겠습니다."},{"metadata":{"_uuid":"6442a1b91a6a7746c3488fcdc5b0adaa4b8df68d"},"cell_type":"markdown","source":"### **- grade(집의 등급)**"},{"metadata":{"trusted":true,"_uuid":"4e549c92870e53e3de5dc4fc635bd8bcf75e16f6"},"cell_type":"code","source":"data = pd.concat([train['price'], train['grade']], axis = 1)\nf, ax = plt.subplots(figsize = (8, 6))\nfig = sns.boxplot(x = 'grade', y = 'price', data = data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12cd7e57d5da88ecb356363749f9ba14b8f41b7a"},"cell_type":"markdown","source":"* 3등급의 경우, 4등급의 Q3보다 높은 평균을 보입니다.\n* 5등급 - 11등급까지 상위 이상치가 다수 존재하며, 11등급은 유독 떨어진 이상치가 존재합니다.\n* 6 - 10까지는 하위 이상치 또한 존재합니다."},{"metadata":{"_uuid":"94e5f756ec8fca276987cbd62dda5b9130f2cd19"},"cell_type":"markdown","source":"### ** - sqft_living(주거 공간의 평방 피트(면적)) **"},{"metadata":{"trusted":true,"_uuid":"a3ade68b07b7a03bc084054263ae3f464ff6fa6b"},"cell_type":"code","source":"data = pd.concat([train['price'], train['sqft_living']], axis = 1)\nf, ax = plt.subplots(figsize = (8, 6))\nfig = sns.regplot(x = 'sqft_living', y = 'price', data = data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae9ed15a371d0eef0e21065e7d1db635e9bb38bd"},"cell_type":"markdown","source":"* sqft_living이 12000인 값까지 면적 상승에 따른 가격의 상승을 따르지만, 14000인 값은 면적이 더욱 상승되었으나 가격이 12000보다 쌉니다."},{"metadata":{"_uuid":"fefee4f32db9eaf0c94d78b2d9de0946dc0b2dd7"},"cell_type":"markdown","source":"### ** - sqft_living15 ** (2015년 기준 주거 공간의 평방 피트)"},{"metadata":{"trusted":true,"_uuid":"327ed579705e8777ae74ca5178c4d73d3c9dd25e"},"cell_type":"code","source":"data = pd.concat([train['price'], train['sqft_living15']], axis = 1)\nf, ax = plt.subplots(figsize = (8, 6))\nfig = sns.regplot(x = 'sqft_living15', y = 'price', data = data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24061d7877f975acf8d297c8467121a10ba65afb"},"cell_type":"markdown","source":"* 재건축으로 인해 sqft_living보다 분산이 크지만, 데이터들이 경향성을 잘 따르는 듯 합니다."},{"metadata":{"_uuid":"dd00cbedb2297ae83ed7c624c18cfd32328a5f67"},"cell_type":"markdown","source":"### ** - sqft_above(지하실을 제외한 평방 피트) **"},{"metadata":{"trusted":true,"_uuid":"10fc22d755fd6930412b9319db44f5425accf5dd"},"cell_type":"code","source":"data = pd.concat([train['price'], train['sqft_above']], axis = 1)\nf, ax = plt.subplots(figsize = (8, 6))\nfig = sns.regplot(x = 'sqft_above', y = 'price', data = data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dea81fbd64366bfabace919934e9b2ea11fd0558"},"cell_type":"markdown","source":"* sqft_above의 크기에 따라 price도 높아지는 경향성을 따르고 있는 듯 보입니다. "},{"metadata":{"_uuid":"22f928e7c94a39db1e0ad36488d242fbfe2951e9"},"cell_type":"markdown","source":"### ** - bathrooms(화장실의 수) **"},{"metadata":{"trusted":true,"_uuid":"80768a77b35674dc43838f8a22d31a3b31eea092"},"cell_type":"code","source":"data = pd.concat([train['price'], train['bathrooms']], axis=1)\nf, ax = plt.subplots(figsize=(18, 6))\nfig = sns.boxplot(x='bathrooms', y=\"price\", data=data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8bbe15ded1dc85f8a6668a8cc0b1833ea87c63a"},"cell_type":"markdown","source":"bathrooms같은경우 소숫점이 있어서 많이 헷갈릴 수 있습니다. 각각의 값이 의미하는것은 아래와 같고 위의 값만을 가지고 아래의 값들이 더해져 최종 bathrooms를 이루고 있기에 각각의 값이 몇개있는지는 구분하기 힘들어 보입니다.\n- 0.5 : 세면대, 화장실\n- 0.75 : 세면대, 화장실, 샤워실\n- 1 : 세면대, 화장실, 샤워실, 욕조"},{"metadata":{"_uuid":"0983138e1492db289199fc814286a28bdb7eaa93"},"cell_type":"markdown","source":"### ** - bedrooms(침실의 수)**"},{"metadata":{"trusted":true,"_uuid":"636c2f4d57a9677da83022f03f0d48703c953393"},"cell_type":"code","source":"data = pd.concat([train['price'], train['bedrooms']], axis=1)\nf, ax = plt.subplots(figsize=(18, 6))\nfig = sns.boxplot(x='bedrooms', y=\"price\", data=data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edb7d628ca1e71a4970eef717c85cc4aaabfbffe"},"cell_type":"markdown","source":"* bedrooms 변수의 값이 2 - 6에서 상위 이상치가, 1 - 3에서 하위 이상치가 다수 존재합니다."},{"metadata":{"trusted":true,"_uuid":"0d87bc306774e32bd35693a0fda54684e2f96cbe"},"cell_type":"code","source":"test[test['bedrooms'] > 10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61154d3224367bb715680e8d582ec54e832cbde7"},"cell_type":"markdown","source":"* bedrooms 변수의 특이한 점은, train 데이터는 0-10까지 존재하나 test 데이터는 11, 33값을 갖는 row가 존재한다는 점입니다."},{"metadata":{"_uuid":"6767af56783edf38ccd15b14f80c89ead4b0a6f3"},"cell_type":"markdown","source":"### ** - floors **"},{"metadata":{"trusted":true,"_uuid":"60dfcae29cf218e84348da1c1780a67922c94435"},"cell_type":"code","source":"data = pd.concat([train['price'], train['floors']], axis=1)\nf, ax = plt.subplots(figsize=(18, 6))\nfig = sns.boxplot(x='floors', y=\"price\", data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"또한, 앞선 Data Fields와 train.head()를 통해 보았던 sqft_living, sqft_lot의 값과 해당 변수들의 15년도 값의 차이를 체크해보고자 합니다.\n앞에서 언급했던 내용은 다음과 같습니다.\n  * sqft_living에서 sqft_living15로 가며 면적이 다소 증가함이 보입니다.\n  * sqft_lot은 sqft_living과 다르게 sqft_lot15로 가며 다소 감소했음을 알 수 있습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['sqft_living_gap'] = train['sqft_living15'] - train['sqft_living']\ntrain['sqft_lot_gap'] = train['sqft_lot15'] - train['sqft_lot']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ** - sqft_living_gap(15년 이전과 이후 주거면적의 차이) **"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['sqft_living_gap'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train['price'], train['sqft_living_gap']], axis = 1)\nf, ax = plt.subplots(figsize = (8,6))\nfig = sns.regplot(x = 'sqft_living_gap', y = 'price', data = data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"그래프를 보았을 때 감소하는 경향이 있는 것으로 보이며 그래프와 describe를 통해 보았을 때 15년도의 주거공간 평방 피트로 변화하며 증가한 값들 보다 크게 감소한 값들이 더욱 눈에 띕니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['sqft_lot_gap'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train['price'], train['sqft_lot_gap']], axis = 1)\nf, ax = plt.subplots(figsize = (8,6))\nfig = sns.regplot(x = 'sqft_lot_gap', y = 'price', data = data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"데이터가 0에 매우 몰려있으며, 그래프 상 감소하는 경향을 표하고 있으나  어떠한 경향성이 있다고 파악하기는 다소 힘들어 보입니다.\n해당 변수들은 파생변수로서 유지 후, 모델에서의 importance를 보도록 하겠습니다."},{"metadata":{"_uuid":"739dcee60740ab77faa4a8e87213322d6ef9bac8"},"cell_type":"markdown","source":"## **<a id='3'> 3. 데이터 전처리 </a>**"},{"metadata":{"_uuid":"3c9d979bd9cdc4acbda93d5159652f29b9549ddb"},"cell_type":"markdown","source":"### **<a id='32'> 3-1. 독립변수(X) </a>**"},{"metadata":{"_uuid":"4ac36d6e42234af18d52d6c0c7fc288ab4c6b55b"},"cell_type":"markdown","source":"### **- sqft_living(주거 공간의 평방 피트(면적))**"},{"metadata":{"trusted":true,"_uuid":"7c15dc583f8954e02385b5686d1bd6757b5d97e9"},"cell_type":"code","source":"data = pd.concat([train['price'], train['sqft_living']], axis = 1)\nf, ax = plt.subplots(figsize = (8,6))\nfig = sns.regplot(x = 'sqft_living', y = 'price', data = data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d9e582804256542ccfd3c9c607aaeddfbbe8372","scrolled":false},"cell_type":"code","source":"sorted(set(test['sqft_living']) - set(train['sqft_living']), reverse = True)[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e48b60db15906dd5db097a04b7429809b02807a"},"cell_type":"markdown","source":"* Test에 있으며 Train에 없는 sqft_living의 값을 체크"},{"metadata":{"trusted":true,"_uuid":"159968a32b73101f4c45e22df1bcd08dd4053eda"},"cell_type":"code","source":"max(test['sqft_living'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0afca93621973761901a557f4dc8719162ff409"},"cell_type":"markdown","source":"* test 데이터 중 sqft_living의 가장 큰 값이 9640이므로, 해당 크기에 비해 다소 차이가 있는 sqft_living이 12000이상인 2가지 값을 이상치로 판단하여 제거하고자 함"},{"metadata":{"trusted":true,"_uuid":"4899c906b10cbaa0ffb5393359210100220be49e"},"cell_type":"code","source":"train.loc[train['sqft_living'] > 12000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5df3e0e4e7910087876b1215c0f103f983e296f"},"cell_type":"code","source":"train = train.loc[(train['id'] != 5108) & (train['id'] != 8912),]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a400d7508e193fe4886cc72ac0935d1f4c52ea53"},"cell_type":"markdown","source":"### **- grade(집의 등급)**"},{"metadata":{"trusted":true,"_uuid":"4259927fbdf525d2c13cc20945bc60caa50f3c01"},"cell_type":"code","source":"data = pd.concat([train['price'], train['grade']], axis = 1)\nf, ax = plt.subplots(figsize = (8,6))\nfig = sns.boxplot(x = 'grade', y = 'price', data = data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"110dcab2eca83aa4341f0246f49a8519d4bdb4f5"},"cell_type":"code","source":"train[train['grade'] == 3]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f92a2f716a009fb5f32d6d9c2547161511b9dff3"},"cell_type":"markdown","source":"* grade가 3인 2개의 row는 yr_built가 1920, 1950년으로 오래되었으며 재건축되지 않았고 sqft_lot, sqft_lot15 값이 큽니다."},{"metadata":{"trusted":true,"_uuid":"875c91531fb1b37bdd7a50e75dbd4b1e2a1885e7"},"cell_type":"code","source":"train.loc[(train['price']>14.5) & (train['grade'] == 7)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b038264df4b145c545047f610ef3d421ca6ff54"},"cell_type":"markdown","source":"* grade 7중, 가장 위 떨어진 id 12346을 이상치로 판단하여 제거하고자 합니다"},{"metadata":{"trusted":true,"_uuid":"fe6cbc3c9d00b43d41299efd073c6e0a9a82bd17"},"cell_type":"code","source":"train.loc[(train['price']>14.5) & (train['grade'] == 8)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21049ffdd4c70f676224b4a5e6c38241f9043ded"},"cell_type":"markdown","source":"* grade 8 중에서는, 가장 위에 떨어진 price가 14.8이상인 id 7173을 이상치로 판단하여 제거하고자 함"},{"metadata":{"trusted":true,"_uuid":"e9c256539f0ae270c78def48f30239437e674dfc"},"cell_type":"code","source":"train.loc[(train['price'] > 15.5) & (train['grade'] == 11)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fef712b811d5c901c80eed8c376a5b268671796d"},"cell_type":"markdown","source":"* grade == 11중, 가장 위 동떨어진 id 2775 또한 이상치로 판단하여 제거하고자 함"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.loc[(train['id'] != 12346) & (train['id'] != 7173) & (train['id'] != 2775)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"826190f6d5c711bab6f030030589844884462bfc"},"cell_type":"markdown","source":"### **- sqft_lot(부지 평방피트)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize = (8, 6))\nsns.distplot(train['sqft_lot'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"sqft_lot 변수 또한 지나치게 치우친 경향이 있기에 log transformation을 취해 skewness의 완화가 필요해 보입니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize = (8, 6))\nsns.distplot(sp.special.log1p(train['sqft_lot']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"log transformation 이전에 비해 완화되었음을 확인할 수 있습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train['price'], sp.special.log1p(train['sqft_lot'])], axis = 1)\nf, ax = plt.subplots(figsize = (8,6))\nfig = sns.regplot(x = 'sqft_lot', y = 'price', data = data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4278c969ecbabc201542c2c3a2baa16d7cc0248"},"cell_type":"markdown","source":"## 추가 정규화\n코드는 해당 링크를 활용하였습니다. https://www.kaggle.com/kcs93023/2019-ml-month-2nd-baseline\n\n또한, 코드를 추가 및 수정하며 Data Fields에서 말씀드린 점을 파생변수로 만들어서 체크해보고자 하였습니다.\n\n파생변수중 일부는 https://www.kaggle.com/marchen911/xgboost-lightgbm-catboost 를 참조하였습니다."},{"metadata":{},"cell_type":"markdown","source":"변수 수정"},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train,test]:\n    df['year'] = df.date.apply(lambda x: x[0:4]).astype(int)\n    df['month'] = df.date.apply(lambda x: x[4:6]).astype(int)\n    df['day'] = df.date.apply(lambda x: x[6:8]).astype(int)\n    df['date'] = df['date'].apply(lambda x: x[0:8])\n    df['yr_renovated'] = df['yr_renovated'].apply(lambda x: sp.nan if x == 0 else x)\n    df['yr_renovated'] = df['yr_renovated'].fillna(df['yr_built'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2128af356be25d0661a71ffa95db566ad9dfbb60"},"cell_type":"markdown","source":"#### 변수 생성"},{"metadata":{"trusted":true,"_uuid":"576793674a946f04c7e2001a501a08d4ad42cc62"},"cell_type":"code","source":"for df in [train,test]:\n    # bedrooms와 bathrooms의 개수를 종합한 방의 개수\n    df['total_rooms'] = df['bedrooms'] + df['bathrooms']\n    \n    # grade와 condition의 곱을 통한 새로운 지표 생성\n    df['grade_condition'] = df['grade'] * df['condition']\n    \n    # 면적관련 변수들을 이용한 파생변수 생성\n    \n    # 15년 변화 이전 부지와 주거공간의 평방피트 합\n    df['sqft_total'] = df['sqft_living'] + df['sqft_lot'] \n    \n    # 15년 변화 이전 부지, 주거공간, 지하실 제외, 지하실의 평방피트 합\n    df['sqft_total_size'] = df['sqft_living'] + df['sqft_lot'] + df['sqft_above'] + df['sqft_basement']\n    \n    # 15년 변화 이후 부지와 주거공간의 평방피트 합\n    df['sqft_total15'] = df['sqft_living15'] + df['sqft_lot15'] \n    \n    # 주거공간 평방피트의 변화 이전과 변화 이후의 차이\n    df['sqft_living_gap'] = sp.absolute(df['sqft_living15'] - df['sqft_living'])\n    \n    # 15년 변화 이후 부지 평방피트의 차이\n    df['sqft_lot_gap'] = sp.absolute(df['sqft_lot15'] - df['sqft_lot'])\n    \n    # 재건축 여부를 통한 파생변수 생성\n    df['is_renovated'] = df['yr_renovated'] - df['yr_built']\n    df['is_renovated'] = df['is_renovated'].apply(lambda x: 0 if x == 0 else 1)\n    df['date'] = df['date'].astype('int')\n    \n    df['garret'] = (df.floors%1==0.5).astype(int)\n    df['diff_of_rooms'] = sp.absolute(df['bedrooms'] - df['bathrooms'])\n    df['living_per_floors'] = df['sqft_living'] / df['floors']\n    df['total_score'] = df['condition'] + df['grade'] + df['view']\n    df['living_per_lot'] = df['sqft_living'] / df['sqft_lot']\n    df['gap_living_per_floor'] = df['sqft_living_gap'] / df['floors']\n    df['exist_special'] = df.garret + df.waterfront + df.is_renovated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb7773c1b9148354a8eac1161ed434ffcb9d3dd8"},"cell_type":"code","source":"train['per_price'] = train['price']/train['sqft_total_size']\nzipcode_price = train.groupby(['zipcode'])['per_price'].agg({'mean','var'}).reset_index()\ntrain = pd.merge(train,zipcode_price,how='left',on='zipcode')\ntest = pd.merge(test,zipcode_price,how='left',on='zipcode')\n\nfor df in [train,test]:\n    df['zipcode_mean'] = df['mean'] * df['sqft_total_size']\n    df['zipcode_var'] = df['var'] * df['sqft_total_size']\n    del df['mean']; del df['var']\n    \ndel train['per_price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transformation\nskew_columns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement']\n\nfor c in skew_columns:\n    train[c] = sp.special.log1p(train[c].values)\n    test[c] = sp.special.log1p(test[c].values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b210b9f9db7784500e90c6c950589bcf7d72227"},"cell_type":"markdown","source":"## **<a id='5'> 5. 모델링 </a>**"},{"metadata":{"_uuid":"acc9ae2e24cf2cf96b863cfa611d02308ce45ab0"},"cell_type":"markdown","source":"#### **<a id='51'>5-1. Linear Regression</a>**"},{"metadata":{"trusted":true,"_uuid":"02979268f89dd89faae0d747fae33a67fb86b47f"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Linear Regression을 사용할 경우, 범주형 변수(categorical variable)에 대해서 더미변수(dummy variable)로 변환하여야 합니다. \n* bedrooms, bathrooms, floors, view, condition, grade, total_rooms, grade_condition, exist_special, living_per_lot, living_per_floors, gap_living_per_floor, total_score, diff_of_rooms 변수들은 데이터 값의 크기에 따른 영향이 있다고 판단하여 label encoder를 적용해주도록 하겠습니다.\n* waterfront, zipcode, yr_built, yr_renovated, garret, is_renovated 변수들에 대하여 pandas 라이브러리 내 get_dummies 함수를 이용하여 더미변수화 하도록 하겠습니다.\n* date의 경우, year, month, day로 더미변수를 생성하였으므로 이를 이용해 년, 월, 일로 나누어 더미변수로 변환 후 모델 성능을 비교해보려 합니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_features_dummy = pd.get_dummies(train, columns = ['waterfront', 'zipcode', 'yr_built', 'yr_renovated', 'year', 'month', 'day', 'garret', 'is_renovated'])\nY_features_dummy = pd.get_dummies(test, columns = ['waterfront', 'zipcode', 'yr_built', 'yr_renovated', 'year', 'month', 'day', 'garret', 'is_renovated'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle_columns = ['bedrooms', 'bathrooms', 'floors', 'view', 'condition', 'grade', 'total_rooms', 'grade_condition', 'exist_special', 'living_per_lot', 'living_per_floors',\n              'gap_living_per_floor', 'total_score', 'diff_of_rooms']\n\n\nle = LabelEncoder()\n\nfor i in le_columns : \n    X_features_dummy[i] = le.fit_transform(X_features_dummy[i])\n    Y_features_dummy[i] = le.fit_transform(Y_features_dummy[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ntest_id = Y_features_dummy['id']\nY_test = Y_features_dummy.drop(['id'], axis = 1, inplace = False)\ny_target = X_features_dummy['price']\nX_data = X_features_dummy.drop(['price', 'id'], axis = 1, inplace = False)\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size = 0.25, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = sp.special.expm1(lr.predict(X_test))\ny_test = sp.special.expm1(y_test)\nmse = mean_squared_error(y_test, pred)\nrmse = mse ** float(0.5)\nprint('RMSE : {0:.3F}'.format(rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 시각화 및 설명의 용이성을 위해 OLS(Ordinary Least Square)를 통해 보고자 합니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.formula.api as sm\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\n\ny = X_features_dummy['price']\nX = X_features_dummy.drop(['price', 'id'], axis = 1, inplace = False)\ntests = Y_features_dummy\n\n# X_features_dummy 데이터 내, 모든 X를 이용하여 Y인 price를 예측하도록 함\nresult = sm.OLS(y, X).fit()\n\nprint(result.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"다중공선성(multicollinearity problem)이 존재함을 Warnings에서 말해주고 있습니다.\n\n선형회귀를 사용할 경우, 기본적인 4가지의 기본 가정이 있습니다.\n\n>   1. **선형성** \n        * 예측하고자 하는 y와 독립변수 x간의 선형성 만족\n   \n>   2. **독립성**\n        * 독립변수 x간 서로 독립성을 만족\n    \n>   3. **등분산성**\n    \n>   4. **정규성**\n        * 잔차의 정규성 만족\n\nmulticollinearity의 존재유무로 인해, 독립성의 가정이 지켜지지 않음을 알 수 있습니다. 또한, 이러한 가정에 대해 자세히 보고자 합니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# fitted values\nmodel_fitted_y = result.fittedvalues\n\n# model residuals\nmodel_residuals = result.resid\n\n# normalized residuals\nmodel_norm_residuals = result.get_influence().resid_studentized_internal\n\n# absolute squared normalized residuals\nmodel_norm_residuals_abs_sqrt = sp.sqrt(sp.absolute(model_norm_residuals))\n\n# absolute residuals\nmodel_abs_resid = sp.absolute(model_residuals)\n\n# leverage, from statsmodel internals\nmodel_leverage = result.get_influence().hat_matrix_diag\n\n# cook's distance, from sttasmodels internals\nmodel_cooks = result.get_influence().cooks_distance[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Residual Plot**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_lm_1 = plt.figure(1)\nplot_lm_1.set_figheight(8)\nplot_lm_1.set_figwidth(12)\n\nplot_lm_1.axes[0] = sns.residplot(model_fitted_y, 'price', data=X_features_dummy, \n                          lowess=True, \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_1.axes[0].set_title('Residuals vs Fitted')\nplot_lm_1.axes[0].set_xlabel('Fitted values')\nplot_lm_1.axes[0].set_ylabel('Residuals')\n\n# annotations\nabs_resid = model_abs_resid.sort_values(ascending=False)\nabs_resid_top_3 = abs_resid[:3]\n\nfor i in abs_resid_top_3.index:\n    plot_lm_1.axes[0].annotate(i, \n                               xy=(model_fitted_y[i], \n                                   model_residuals[i]));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Residuals vs Fitted Plot은 x축에 $\\hat{Y}$, y축에 잔차 ${\\epsilon }_{i}$로 출력합니다. 선형회귀에서 오차는 평균이 0이고 분산이 일정한 정규분포를 가정하기에 $\\hat{Y}$과 무관하게 잔차의 평균은 0을 중심으로 일정하게 패턴 없이 분포되어 있어야 합니다."},{"metadata":{},"cell_type":"markdown","source":"## **QQ Plot**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.gofplots import ProbPlot\n\nQQ = ProbPlot(model_norm_residuals)\nplot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n\nplot_lm_2.set_figheight(8)\nplot_lm_2.set_figwidth(12)\n\nplot_lm_2.axes[0].set_title('Normal Q-Q')\nplot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\nplot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n\n# annotations\nabs_norm_resid = sp.flip(sp.argsort(sp.absolute(model_norm_residuals)), 0)\nabs_norm_resid_top_3 = abs_norm_resid[:3]\n\nfor r, i in enumerate(abs_norm_resid_top_3):\n    plot_lm_2.axes[0].annotate(i, \n                               xy=(sp.flip(QQ.theoretical_quantiles, 0)[r],\n                                   model_norm_residuals[i]));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"잔차의 정규성이 이루어지지 않음을 확인할 수 있습니다. model의 normalized residual값이 -2 이하부터 정규성을 갖추지 못하고 있음을 알 수 있습니다."},{"metadata":{},"cell_type":"markdown","source":"## **Scale-Location Plot**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_lm_3 = plt.figure(3)\nplot_lm_3.set_figheight(8)\nplot_lm_3.set_figwidth(12)\n\nplt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5)\nsns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_3.axes[0].set_title('Scale-Location')\nplot_lm_3.axes[0].set_xlabel('Fitted values')\nplot_lm_3.axes[0].set_ylabel('$\\sqrt{|Standardized Residuals|}$');\n\n# annotations\nabs_sq_norm_resid = sp.flip(sp.argsort(model_norm_residuals_abs_sqrt), 0)\nabs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3]\n\nfor i in abs_norm_resid_top_3:\n    plot_lm_3.axes[0].annotate(i, \n                               xy=(model_fitted_y[i], \n                                   model_norm_residuals_abs_sqrt[i]));\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scale-location plot은 x축에 $\\hat{Y}$, y축에는 표준화 잔차(Standard Residuals)를 보입니다. 빨간 추세선이 가장 이상적이며, 벗어난 값들은 이상치(outlier)일 가능성이 있습니다."},{"metadata":{},"cell_type":"markdown","source":"## **Leverage Plot**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_lm_4 = plt.figure(4)\nplot_lm_4.set_figheight(8)\nplot_lm_4.set_figwidth(12)\n\nplt.scatter(model_leverage, model_norm_residuals, alpha=0.5)\nsns.regplot(model_leverage, model_norm_residuals, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_4.axes[0].set_xlim(0, 0.20)\nplot_lm_4.axes[0].set_ylim(-3, 5)\nplot_lm_4.axes[0].set_title('Residuals vs Leverage')\nplot_lm_4.axes[0].set_xlabel('Leverage')\nplot_lm_4.axes[0].set_ylabel('Standardized Residuals')\n\n# annotations\nleverage_top_3 = sp.flip(sp.argsort(model_cooks), 0)[:3]\n\nfor i in leverage_top_3:\n    plot_lm_4.axes[0].annotate(i, \n                               xy=(model_leverage[i], \n                                   model_norm_residuals[i]))\n    \n# shenanigans for cook's distance contours\ndef graph(formula, x_range, label=None):\n    x = x_range\n    y = formula(x)\n    plt.plot(x, y, label=label, lw=1, ls='--', color='red')\n\np = len(result.params) # number of model parameters\n\ngraph(lambda x: sp.sqrt((0.5 * p * (1 - x)) / x), \n      sp.linspace(0.001, 0.200, 50), \n      'Cook\\'s distance') # 0.5 line\ngraph(lambda x: sp.sqrt((1 * p * (1 - x)) / x), \n      sp.linspace(0.001, 0.200, 50)) # 1 line\nplt.legend(loc='upper right');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Leverage는 설명변수가 얼마나 극단에 치우쳐 있는지 말합니다."},{"metadata":{},"cell_type":"markdown","source":"### **위에서 본 바와 같이**\n선형회귀는 기본 4가지 가정이 모두 충족되어야 하기에 독립성 가정부터 만족되어야 합니다. 이를 위해, 다중공선성을 제거하고자 사용할 수 있는 방법은 (1) VIF 값을 기준으로 공선성을 야기시키는 변수를 제거, (2) 데이터의 row를 늘려줌으로써 표본의 크기를 증가, (3) 변수의 변환 등이 있습니다. \n\n하지만, competition 내에서 데이터의 row를 늘려줌은 불가능하며 VIF값을 기준으로 변수들을 제거해 나아가는 점은 기존 변수들을 이용해 생성한 파생변수를 사용하지 못하게 될 문제들이 있습니다. 그렇기에, 다중공선성에 robust하며 평균적으로 성능이 좋은 Tree 기반 모델을 사용합니다.\n\n저 또한, 이후 Tree 기반 모델들을 사용하여 예측해보고자 합니다."},{"metadata":{"_uuid":"b59eec3de653c78ca2a07b6c57d20b18720c8ee3"},"cell_type":"markdown","source":"\n#### **<a id='52'> 5-2. Random Forest </a>**"},{"metadata":{"trusted":true,"_uuid":"691d90684cb5f0c9f86d6ac075a473858f779f7a"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = test['id']\nY_test = test.drop(['id'], axis = 1, inplace = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_target = train['price']\nX_data = train.drop(['price', 'id'], axis = 1, inplace = False)\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size = 0.25, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c55bdc00291ac6752e02d7848266c470dc526667"},"cell_type":"code","source":"forest_reg = RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_dist = {\"max_depth\": [7, 11, 15, 18, 21],\n              \"max_features\": sp_randint(1, 31),\n              \"min_samples_split\": sp_randint(2, 21),\n              \"min_samples_leaf\": sp_randint(1, 21),\n              \"bootstrap\": [True, False],\n              \"random_state\": [42]\n             }\n\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(forest_reg, param_distributions=param_dist,\n                                   n_iter=n_iter_search)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report(random_search.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"randomizedsearch를 통해 높은 score를 보이는 parameter를 파악하였습니다."},{"metadata":{"trusted":true,"_uuid":"8ddbff511ffd4cf9858861b1a436690553e6e9de"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error as mse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = sp.special.expm1(random_search.predict(X_test))\ny_test = sp.expm1(y_test)\nrmse = (mean_squared_error(y_test, pred)) ** float(0.5)\nprint('RMSE : {0:.3F}'.format(rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = sp.special.expm1(random_search.predict(Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cf5b9247357a60c31fa06429883ef7049236149"},"cell_type":"code","source":"submission = pd.DataFrame({'id' : test_id, 'price' : test_pred})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ff5fe937751d6c0461fe3c6af6d9746b70637f7"},"cell_type":"code","source":"submission.to_csv('rfgridcv.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n#### **<a id='53'> 5-3. XGBoost </a>**\n"},{"metadata":{},"cell_type":"markdown","source":"XGBoost의 설명은 다음 article을 참조하였습니다. \n\nhttps://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"},{"metadata":{},"cell_type":"markdown","source":"-------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"### **XGBoost Advantage**"},{"metadata":{},"cell_type":"markdown","source":"1. **Regularization** : Standard GBM(Gradient Boosting Machine)은 overfitting을 줄이는 데 도움이 됩니다.\n\n2. **Parallel Processing** : XGBoost는 병렬 처리를 구현하며 GBM과 비교해 매우 빠릅니다.\n\n3. **High Flexibility** : XGBoost는 유저가 custom optimization objectives와 평가 지표(evaluation criteria)를 정의할 수 있습니다.\n\n4. **Handling Missing Values** : XGBoost는 missing value를 핸들링하는 routine이 in-built되어 있습니다.\n\n5. **Tree Pruning** : GBM은 greedy algorithm과 같이, negative loss가 발생하면 node split을 중단함에 반해, XGBoost는 지정된 max_depth까지 split하고 트리를 거꾸로 잘라 positive gain이 없으면 pruning을 합니다.\n\n6. **Built-in Cross-Validation** : XGBoost는 유저가 매 iteration마다 cross-validation을 실행할 수 있도록 합니다.\n\n7. **Continue on Existing Model** : 이전에 실행시킨 last iteration으로부터 XGBoost model을 training시킬 수 있습니다."},{"metadata":{},"cell_type":"markdown","source":"\n* XGBoost는 해당 커널을 참조하여 작성토록 했음을 명시합니다. https://www.kaggle.com/marchen911/xgboost-lightgbm-catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn import model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = test['id']\nY_test = test.drop(['id'], axis = 1, inplace = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_target = train['price']\nX_data = train.drop(['price', 'id'], axis = 1, inplace = False)\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size = 0.25, random_state = 42)\nwatchlist=[(X_train,y_train),(X_test,y_test)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature missmatch 에러를 해결하기 위함\nY_test = Y_test[X_test.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model= XGBRegressor(tree_method='gpu_hist',\n                        n_estimators=100000,\n                        num_round_boost=500,\n                        show_stdv=False,\n                        feature_selector='greedy',\n                        verbosity=0,\n                        reg_lambda=10,\n                        reg_alpha=0.01,\n                        learning_rate=0.001,\n                        seed=42,\n                        colsample_bytree=0.8,\n                        colsample_bylevel=0.8,\n                        subsample=0.8,\n                        n_jobs=-1,\n                        gamma=0.005,\n                        base_score=np.mean(y_target)\n                        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model.fit(X_train,y_train, verbose=False, eval_set=watchlist,\n              eval_metric='rmse',\n              early_stopping_rounds=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_score=mse(sp.special.expm1(xgb_model.predict(X_test)),sp.special.expm1(y_test))**0.5\n\nprint(\"RMSE : {}\".format(xgb_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_pred = sp.special.expm1(xgb_model.predict(Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.read_csv('../input/sample_submission.csv')\nsubmission['price']= xgb_pred\nsubmission.to_csv('xgb_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n#### **<a id='54'> 5-4. LightGBM </a>**"},{"metadata":{},"cell_type":"markdown","source":"lightGBM모델은 해당 커널을 참조하여 작성하였음을 명시합니다. https://www.kaggle.com/marchen911/xgboost-lightgbm-catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_target = train['price']\nX_data = train.drop(['price', 'id'], axis = 1, inplace = False)\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size = 0.2, random_state = 42)\n\nmodel_lgb=lgb.LGBMRegressor(\n                           learning_rate=0.001,\n                           n_estimators=100000,\n                           subsample=0.6,\n                           colsample_bytree=0.6,\n                           reg_alpha=0.2,\n                           reg_lambda=10,\n                           num_leaves=35,\n                           silent=True,\n                           min_child_samples=10,\n                            \n                           )\n\nmodel_lgb.fit(X_train,y_train,eval_set=(X_test,y_test),verbose=0,early_stopping_rounds=1000,\n              eval_metric='rmse')\n\nlgb_score=mse(sp.special.expm1(model_lgb.predict(X_test)),sp.special.expm1(y_test))**0.5\nprint(\"RMSE unseen : {}\".format(lgb_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_pred = sp.special.expm1(model_lgb.predict(Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.read_csv('../input/sample_submission.csv')\nsubmission['price']= lgb_pred\nsubmission.to_csv('lgbm_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ensemble**"},{"metadata":{"trusted":true},"cell_type":"code","source":"score=lgb_score+xgb_score\nlgb_ratio=1-lgb_score/score\nxgb_ratio=1-xgb_score/score\npredict=lgb_pred*(lgb_ratio)+xgb_pred*(xgb_ratio)\nprint('lgb_ratio={}, xgb_ratio={}'.format(lgb_ratio,xgb_ratio))\nsubmission=pd.read_csv('../input/sample_submission.csv')\nsubmission.loc[:,'price']=predict\nsubmission.to_csv('xgb_lgbm.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}